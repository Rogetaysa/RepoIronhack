{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL base d'Ironman (ajusta segons el lloc web)\n",
    "BASE_URL = \"https://www.ironman.com\"\n",
    "\n",
    "def get_race_info(race_url):\n",
    "    \"\"\"Funció per obtenir la informació d'una cursa concreta.\"\"\"\n",
    "    response = requests.get(race_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Aconsegueix informació específica (ajusta segons la pàgina)\n",
    "    data = {}\n",
    "    try:\n",
    "        data['Swim'] = soup.find('div', text='Swim').find_next('p').text.strip()\n",
    "        data['Bike'] = soup.find('div', text='Bike').find_next('p').text.strip()\n",
    "        data['Run'] = soup.find('div', text='Run').find_next('p').text.strip()\n",
    "        data['Avg. Air Temp'] = soup.find('div', text='Avg. Air Temp').find_next('p').text.strip()\n",
    "        data['Avg. Water Temp'] = soup.find('div', text='Avg. Water Temp').find_next('p').text.strip()\n",
    "        data['Bike Altitude'] = soup.find('div', text='Bike Altitude').find_next('p').text.strip()\n",
    "        data['Run Altitude'] = soup.find('div', text='Run Altitude').find_next('p').text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {race_url}: {e}\")\n",
    "    return data\n",
    "\n",
    "def get_all_races():\n",
    "    \"\"\"Funció per obtenir els enllaços de totes les curses.\"\"\"\n",
    "    races = []\n",
    "    response = requests.get(f\"{BASE_URL}/races\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Troba els enllaços de les curses (ajusta el selector segons el lloc web)\n",
    "    race_links = soup.find_all('a', class_='race-link')  # Ajustar si cal\n",
    "    for link in race_links:\n",
    "        race_url = BASE_URL + link['href']\n",
    "        races.append(race_url)\n",
    "    return races\n",
    "\n",
    "def main():\n",
    "    # Obtenir les curses\n",
    "    race_urls = get_all_races()\n",
    "    \n",
    "    # Extreure informació de cada cursa\n",
    "    race_data = []\n",
    "    for url in race_urls:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        info = get_race_info(url)\n",
    "        if info:\n",
    "            race_data.append(info)\n",
    "    \n",
    "    # Guardar les dades en un CSV\n",
    "    df = pd.DataFrame(race_data)\n",
    "    df.to_csv('ironman_races.csv', index=False)\n",
    "    print(\"Dades guardades a ironman_races.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
